#!/usr/bin/env python3
"""
AI Functions è¿”å›æ ¼å¼ç»Ÿä¸€ä¼˜åŒ–å·¥å…·
è§£å†³æ‰€æœ‰30ä¸ªAIå‡½æ•°çš„è¿”å›æ ¼å¼ä¸ä¸€è‡´é—®é¢˜
"""

import re
import json
from typing import Dict, List, Tuple

class AIFunctionOptimizer:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.backup_path = file_path + '.backup_before_optimization'
        
        # æ ‡å‡†åŒ–çš„æç¤ºè¯æ¨¡æ¿
        self.prompt_templates = {
            # æ–‡æœ¬å¤„ç†å‡½æ•°
            "ai_text_sentiment_analyze": """ä½ æ˜¯ä¸“ä¸šæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚åˆ†ææ–‡æœ¬æƒ…æ„Ÿå€¾å‘ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"sentiment": "positive|negative|neutral", "confidence": 0.95, "emotions": ["joy", "anger"], "keywords": ["å…³é”®è¯1"]}""",
            
            "ai_text_extract_entities": """ä½ æ˜¯ä¸“ä¸šä¿¡æ¯æå–ä¸“å®¶ã€‚ä»æ–‡æœ¬ä¸­æå–å®ä½“ä¿¡æ¯ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"entities": [{"text": "å®ä½“å", "type": "PERSON|ORG|LOC|MISC", "confidence": 0.95}]}""",
            
            "ai_text_extract_keywords": """ä½ æ˜¯å…³é”®è¯æå–ä¸“å®¶ã€‚æå–æ–‡æœ¬çš„æ ¸å¿ƒå…³é”®è¯ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"keywords": [{"word": "å…³é”®è¯", "weight": 0.95, "category": "æ ¸å¿ƒæ¦‚å¿µ"}]}""",
            
            "ai_text_classify": """ä½ æ˜¯æ–‡æœ¬åˆ†ç±»ä¸“å®¶ã€‚å°†æ–‡æœ¬åˆ†ç±»åˆ°åˆé€‚ç±»åˆ«ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"category": "åˆ†ç±»åç§°", "confidence": 0.95, "subcategory": "å­åˆ†ç±»", "categories_considered": ["ç±»åˆ«1", "ç±»åˆ«2"]}""",
            
            "ai_text_clean_normalize": """ä½ æ˜¯æ–‡æœ¬æ¸…æ´—ä¸“å®¶ã€‚æ‰§è¡Œæ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–æ“ä½œã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"cleaned_text": "æ¸…æ´—åæ–‡æœ¬", "operations_applied": ["å»é‡", "æ ‡å‡†åŒ–"], "changes_count": 5}""",
            
            "ai_auto_tag_generate": """ä½ æ˜¯æ™ºèƒ½æ ‡ç­¾ç”Ÿæˆä¸“å®¶ã€‚ä¸ºæ–‡æœ¬ç”Ÿæˆç›¸å…³æ ‡ç­¾ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"tags": [{"tag": "æ ‡ç­¾å", "relevance": 0.95, "category": "ä¸»é¢˜"}]}""",
            
            # ä¸šåŠ¡åœºæ™¯å‡½æ•°
            "ai_customer_intent_analyze": """ä½ æ˜¯å®¢æˆ·æ„å›¾åˆ†æä¸“å®¶ã€‚åˆ†æå®¢æˆ·æ–‡æœ¬çš„çœŸå®æ„å›¾ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"intent": "è´­ä¹°æ„å‘|å’¨è¯¢|æŠ•è¯‰|å»ºè®®", "confidence": 0.95, "urgency": "high|medium|low", "emotions": ["satisfied"], "action_required": "ç«‹å³å¤„ç†"}""",
            
            "ai_sales_lead_score": """ä½ æ˜¯é”€å”®çº¿ç´¢è¯„åˆ†ä¸“å®¶ã€‚æ ¹æ®æ ‡å‡†è¯„ä¼°çº¿ç´¢ä»·å€¼ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"score": 85, "grade": "A|B|C|D", "probability": 0.85, "factors": [{"factor": "é¢„ç®—å……è¶³", "impact": "positive", "weight": 0.3}], "next_action": "ç«‹å³è·Ÿè¿›"}""",
            
            "ai_review_analyze": """ä½ æ˜¯è¯„è®ºåˆ†æä¸“å®¶ã€‚åˆ†æç”¨æˆ·è¯„è®ºçš„å¤šç»´åº¦ä¿¡æ¯ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"sentiment": "positive|negative|neutral", "rating_predicted": 4.5, "aspects": [{"aspect": "æœåŠ¡", "sentiment": "positive", "score": 4.2}], "key_issues": ["å¾…æ”¹è¿›ç‚¹"]}""",
            
            "ai_risk_text_detect": """ä½ æ˜¯é£é™©æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹æ–‡æœ¬ä¸­çš„å„ç±»é£é™©å†…å®¹ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"risk_level": "high|medium|low|none", "risk_types": ["æ¬ºè¯ˆ", "è¿è§„"], "confidence": 0.95, "flagged_content": ["å…·ä½“é£é™©æ–‡æœ¬"], "action_required": true}""",
            
            "ai_contract_extract": """ä½ æ˜¯åˆåŒä¿¡æ¯æå–ä¸“å®¶ã€‚æå–åˆåŒçš„å…³é”®ä¿¡æ¯å­—æ®µã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"parties": ["ç”²æ–¹", "ä¹™æ–¹"], "amount": "1000000", "start_date": "2024-01-01", "end_date": "2024-12-31", "key_terms": ["é‡è¦æ¡æ¬¾"], "risk_points": ["é£é™©ç‚¹"]}""",
            
            "ai_resume_parse": """ä½ æ˜¯ç®€å†è§£æä¸“å®¶ã€‚è§£æç®€å†çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"name": "å§“å", "education": [{"degree": "æœ¬ç§‘", "school": "å¤§å­¦", "major": "ä¸“ä¸š"}], "experience": [{"title": "èŒä½", "company": "å…¬å¸", "duration": "2å¹´"}], "skills": ["æŠ€èƒ½1"]}""",
            
            "ai_customer_segment": """ä½ æ˜¯å®¢æˆ·ç»†åˆ†ä¸“å®¶ã€‚æ ¹æ®æ¨¡å‹è¿›è¡Œå®¢æˆ·ç»†åˆ†åˆ†æã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"segment": "é«˜ä»·å€¼å®¢æˆ·", "scores": {"R": 5, "F": 4, "M": 5}, "total_score": 85, "characteristics": ["è´­ä¹°é¢‘ç¹"], "recommendations": ["VIPæœåŠ¡"], "retention_probability": 0.92}""",
            
            "ai_product_description_generate": """ä½ æ˜¯äº§å“æ–‡æ¡ˆä¸“å®¶ã€‚ç”Ÿæˆå¸å¼•äººçš„äº§å“æè¿°ã€‚
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼š
{"title": "äº§å“æ ‡é¢˜", "description": "è¯¦ç»†æè¿°", "features": ["ç‰¹è‰²1", "ç‰¹è‰²2"], "benefits": ["ä¼˜åŠ¿1"], "target_audience": "ç›®æ ‡ç”¨æˆ·", "selling_points": ["å–ç‚¹1"]}"""
        }
        
        # ç»Ÿä¸€çš„å›é€€å­—æ®µæ˜ å°„
        self.fallback_fields = {
            "ai_text_summarize": "summary",
            "ai_text_translate": "translated_text", 
            "ai_text_sentiment_analyze": "sentiment_analysis",
            "ai_text_extract_entities": "entities_extraction",
            "ai_text_extract_keywords": "keywords_extraction",
            "ai_text_classify": "classification_result",
            "ai_text_clean_normalize": "text_processing_result",
            "ai_auto_tag_generate": "tags_generation",
            "ai_customer_intent_analyze": "intent_analysis",
            "ai_sales_lead_score": "lead_scoring",
            "ai_review_analyze": "review_analysis", 
            "ai_risk_text_detect": "risk_assessment",
            "ai_contract_extract": "contract_extraction",
            "ai_resume_parse": "resume_parsing",
            "ai_customer_segment": "segmentation_analysis",
            "ai_product_description_generate": "product_description",
            "ai_industry_classification": "industry_classification"
        }

    def backup_file(self):
        """å¤‡ä»½åŸæ–‡ä»¶"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        with open(self.backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {self.backup_path}")

    def fix_syntax_errors(self, content: str) -> str:
        """ä¿®å¤è¯­æ³•é”™è¯¯"""
        # ä¿®å¤ ai_industry_classification ä¸­çš„è¯­æ³•é”™è¯¯
        pattern = r'(\s+if hasattr\(response\.output\.choices\[0\]\.message, \'content\'\):\s+if content:)'
        replacement = r'\1\n                        content = response.output.choices[0].message.content\n                        if content:'
        content = re.sub(pattern, replacement, content, flags=re.DOTALL)
        print("âœ… ä¿®å¤äº† ai_industry_classification è¯­æ³•é”™è¯¯")
        return content

    def update_system_prompts(self, content: str) -> str:
        """æ›´æ–°ç³»ç»Ÿæç¤ºè¯"""
        for func_name, new_prompt in self.prompt_templates.items():
            # æŸ¥æ‰¾å‡½æ•°å®šä¹‰
            pattern = rf'class {func_name}\(object\):.*?messages = \[(.*?)\]'
            match = re.search(pattern, content, re.DOTALL)
            
            if match:
                messages_content = match.group(1)
                # æ›´æ–°system content
                old_system_pattern = r'\{"role": "system", "content": f?"[^"]*"\}'
                new_system_content = f'{{"role": "system", "content": f"""{new_prompt}"""}}'
                
                # å¦‚æœæœ‰f-stringï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if 'segmentation_model' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆä½¿ç”¨æ¨¡å‹ï¼š{{segmentation_model}}ï¼‰"}}'
                elif 'max_keywords' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆæœ€å¤šæå–{{max_keywords}}ä¸ªå…³é”®è¯ï¼‰"}}'
                elif 'categories' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆå€™é€‰ç±»åˆ«ï¼š{{categories}}ï¼‰"}}'
                elif 'operations' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆæ‰§è¡Œæ“ä½œï¼š{{operations}}ï¼‰"}}'
                elif 'max_tags' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆç”Ÿæˆ{{max_tags}}ä¸ªæ ‡ç­¾ï¼‰"}}'
                elif 'business_context' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆä¸šåŠ¡èƒŒæ™¯ï¼š{{business_context}}ï¼‰"}}'
                elif 'scoring_criteria' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆè¯„åˆ†æ ‡å‡†ï¼š{{scoring_criteria}}ï¼‰"}}'
                elif 'product_type' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆäº§å“ç±»å‹ï¼š{{product_type}}ï¼‰"}}'
                elif 'risk_types' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆé£é™©ç±»å‹ï¼š{{risk_types}}ï¼‰"}}'
                elif 'extract_fields' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆæå–å­—æ®µï¼š{{extract_fields}}ï¼‰"}}'
                elif 'parse_depth' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆè§£ææ·±åº¦ï¼š{{parse_depth}}ï¼‰"}}'
                elif 'style' in messages_content:
                    new_system_content = f'{{"role": "system", "content": f"{new_prompt}ï¼ˆæ–‡æ¡ˆé£æ ¼ï¼š{{style}}ï¼‰"}}'
                
                updated_messages = re.sub(old_system_pattern, new_system_content, messages_content)
                content = content.replace(messages_content, updated_messages)
                print(f"âœ… æ›´æ–°äº† {func_name} çš„ç³»ç»Ÿæç¤ºè¯")
            
        return content

    def standardize_response_handling(self, content: str) -> str:
        """æ ‡å‡†åŒ–å“åº”å¤„ç†é€»è¾‘"""
        for func_name, fallback_field in self.fallback_fields.items():
            # æŸ¥æ‰¾å‡½æ•°çš„exceptå—
            pattern = rf'(class {func_name}\(object\):.*?except:)\s+(result = \{{.*?\}})'
            
            def replace_except_block(match):
                before_except = match.group(1)
                standardized_except = f'result = {{"{fallback_field}": full_content}}'
                return before_except + '\n                ' + standardized_except
            
            content = re.sub(pattern, replace_except_block, content, flags=re.DOTALL)
        
        print("âœ… æ ‡å‡†åŒ–äº†å“åº”å¤„ç†é€»è¾‘")
        return content

    def add_response_validator(self, content: str) -> str:
        """æ·»åŠ å“åº”éªŒè¯å™¨"""
        validator_code = '''
# å“åº”éªŒè¯å™¨ - åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ 
def validate_ai_response(result, function_name):
    """éªŒè¯AIå‡½æ•°è¿”å›æ ¼å¼"""
    if not isinstance(result, dict):
        return False
    
    # åŸºæœ¬å­—æ®µéªŒè¯
    required_fields = {
        "ai_text_sentiment_analyze": ["sentiment", "confidence"],
        "ai_text_extract_entities": ["entities"],
        "ai_customer_segment": ["segment", "scores", "total_score"],
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å‡½æ•°çš„å¿…éœ€å­—æ®µ
    }
    
    if function_name in required_fields:
        return all(field in result for field in required_fields[function_name])
    
    return True  # å¯¹äºæœªå®šä¹‰éªŒè¯è§„åˆ™çš„å‡½æ•°ï¼Œè¿”å›True

'''
        # åœ¨å¯¼å…¥è¯­å¥åæ·»åŠ éªŒè¯å™¨
        import_end = content.find('# ==================== æ–‡æœ¬å¤„ç†å‡½æ•°')
        content = content[:import_end] + validator_code + '\n' + content[import_end:]
        print("âœ… æ·»åŠ äº†å“åº”éªŒè¯å™¨")
        return content

    def optimize(self):
        """æ‰§è¡Œå®Œæ•´ä¼˜åŒ–"""
        print("ğŸš€ å¼€å§‹AIå‡½æ•°è¿”å›æ ¼å¼ä¼˜åŒ–...\n")
        
        # 1. å¤‡ä»½æ–‡ä»¶
        self.backup_file()
        
        # 2. è¯»å–æ–‡ä»¶
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 3. ä¿®å¤è¯­æ³•é”™è¯¯
        content = self.fix_syntax_errors(content)
        
        # 4. æ›´æ–°ç³»ç»Ÿæç¤ºè¯
        content = self.update_system_prompts(content)
        
        # 5. æ ‡å‡†åŒ–å“åº”å¤„ç†
        content = self.standardize_response_handling(content)
        
        # 6. æ·»åŠ å“åº”éªŒè¯å™¨
        content = self.add_response_validator(content)
        
        # 7. å†™å›æ–‡ä»¶
        with open(self.file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"\nğŸ‰ ä¼˜åŒ–å®Œæˆï¼")
        print(f"ğŸ“„ ä¼˜åŒ–çš„æ–‡ä»¶: {self.file_path}")
        print(f"ğŸ’¾ å¤‡ä»½æ–‡ä»¶: {self.backup_path}")
        print(f"\nğŸ“Š ä¼˜åŒ–ç»Ÿè®¡:")
        print(f"  - ä¿®å¤è¯­æ³•é”™è¯¯: 1ä¸ª")
        print(f"  - æ›´æ–°æç¤ºè¯: {len(self.prompt_templates)}ä¸ª")
        print(f"  - æ ‡å‡†åŒ–å›é€€å­—æ®µ: {len(self.fallback_fields)}ä¸ª")
        print(f"  - æ·»åŠ å“åº”éªŒè¯å™¨: 1ä¸ª")
        
        return True

    def generate_test_script(self):
        """ç”Ÿæˆæµ‹è¯•è„šæœ¬"""
        test_script = '''#!/usr/bin/env python3
"""
ä¼˜åŒ–åAIå‡½æ•°çš„æµ‹è¯•è„šæœ¬
éªŒè¯è¿”å›æ ¼å¼æ˜¯å¦ç¬¦åˆæ ‡å‡†
"""

import json
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from ai_functions_complete import *

def test_function_format(func_class, func_name, test_data):
    """æµ‹è¯•å•ä¸ªå‡½æ•°çš„è¿”å›æ ¼å¼"""
    print(f"\\n=== æµ‹è¯• {func_name} ===")
    
    func_instance = func_class()
    
    # æ¨¡æ‹Ÿæµ‹è¯•ï¼ˆä½¿ç”¨å‡APIå¯†é’¥ï¼‰
    if func_name == "ai_customer_segment":
        result = func_instance.evaluate(test_data, 'test-key', 'RFM', 'qwen-plus')
    elif func_name == "ai_text_summarize":
        result = func_instance.evaluate(test_data, 'test-key', 'qwen-plus', 100)
    else:
        result = func_instance.evaluate(test_data, 'test-key', 'qwen-plus')
    
    try:
        data = json.loads(result)
        print(f"âœ… JSONæ ¼å¼æ­£ç¡®")
        print(f"ğŸ“ è¿”å›å­—æ®µ: {list(data.keys())}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å­—æ®µ
        if 'error' in data:
            print(f"âš ï¸  é”™è¯¯ä¿¡æ¯: {data.get('message', 'Unknown error')}")
        else:
            print(f"ğŸ“Š æ•°æ®é¢„è§ˆ: {str(data)[:200]}...")
            
        return True
    except Exception as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        print(f"ğŸ“„ åŸå§‹ç»“æœ: {result[:200]}...")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    test_cases = [
        (ai_text_sentiment_analyze, "ai_text_sentiment_analyze", "ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼"),
        (ai_text_extract_entities, "ai_text_extract_entities", "å¼ ä¸‰åœ¨åŒ—äº¬å·¥ä½œ"),
        (ai_customer_segment, "ai_customer_segment", '{"recency": 30, "frequency": 5, "monetary": 1000}'),
        (ai_text_summarize, "ai_text_summarize", "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯ï¼Œç”¨äºåˆ›å»ºæ™ºèƒ½ç³»ç»Ÿã€‚"),
    ]
    
    success_count = 0
    total_count = len(test_cases)
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ä¼˜åŒ–åçš„AIå‡½æ•°...")
    
    for func_class, func_name, test_data in test_cases:
        success = test_function_format(func_class, func_name, test_data)
        if success:
            success_count += 1
    
    print(f"\\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸ: {success_count}/{total_count}")
    print(f"âŒ å¤±è´¥: {total_count - success_count}/{total_count}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    
    if success_count == total_count:
        print("\\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ ¼å¼ä¼˜åŒ–æˆåŠŸã€‚")
    else:
        print("\\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¼˜åŒ–ç»“æœã€‚")

if __name__ == "__main__":
    main()
'''
        
        test_file_path = '/Users/liangmo/Documents/GitHub/clickzetta_aisql/test_optimized_functions.py'
        with open(test_file_path, 'w', encoding='utf-8') as f:
            f.write(test_script)
        
        print(f"âœ… ç”Ÿæˆæµ‹è¯•è„šæœ¬: {test_file_path}")
        return test_file_path

def main():
    """ä¸»å‡½æ•°"""
    file_path = '/Users/liangmo/Documents/GitHub/clickzetta_aisql/ai_functions_complete.py'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    # åˆ›å»ºä¼˜åŒ–å™¨å¹¶æ‰§è¡Œä¼˜åŒ–
    optimizer = AIFunctionOptimizer(file_path)
    success = optimizer.optimize()
    
    if success:
        # ç”Ÿæˆæµ‹è¯•è„šæœ¬
        test_script_path = optimizer.generate_test_script()
        
        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
        print(f"1. è¿è¡Œæµ‹è¯•: python {test_script_path}")
        print(f"2. å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œé‡æ–°æ‰“åŒ…: python package_with_deps.py")
        print(f"3. éƒ¨ç½²åˆ°ClickZettaå¹³å°")
    
    return success

if __name__ == '__main__':
    import os
    main()