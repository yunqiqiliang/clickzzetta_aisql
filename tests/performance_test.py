#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•AIå‡½æ•°çš„å¹¶å‘æ€§èƒ½å’Œæ‰¹é‡å¤„ç†èƒ½åŠ›
"""

import json
import sys
import time
import asyncio
import concurrent.futures
from datetime import datetime

sys.path.insert(0, '/Users/liangmo/Documents/GitHub/clickzetta_aisql')

def test_batch_processing(func_name, test_data, api_key):
    """æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½"""
    from ai_functions_complete import ai_text_sentiment_analyze
    
    start_time = time.time()
    results = []
    
    # åˆ›å»ºå‡½æ•°å®ä¾‹
    func = ai_text_sentiment_analyze()
    
    # æ‰¹é‡å¤„ç†
    for text in test_data:
        result = func.evaluate(text=text, api_key=api_key)
        results.append(result)
    
    end_time = time.time()
    
    return {
        "total_time": end_time - start_time,
        "avg_time": (end_time - start_time) / len(test_data),
        "count": len(test_data),
        "results": results
    }


def test_concurrent_requests(func_name, test_data, api_key, max_workers=5):
    """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
    from ai_functions_complete import ai_text_sentiment_analyze
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # åˆ›å»ºå‡½æ•°å®ä¾‹
        func = ai_text_sentiment_analyze()
        
        # æäº¤å¹¶å‘ä»»åŠ¡
        futures = []
        for text in test_data:
            future = executor.submit(func.evaluate, text=text, api_key=api_key)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = []
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())
    
    end_time = time.time()
    
    return {
        "total_time": end_time - start_time,
        "avg_time": (end_time - start_time) / len(test_data),
        "count": len(test_data),
        "max_workers": max_workers
    }


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python performance_test.py YOUR_API_KEY")
        return
    
    api_key = sys.argv[1]
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open('data/batch_test_data.json', 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    reviews = test_data['batch_sentiment']['reviews']
    
    print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # 1. é¡ºåºå¤„ç†æµ‹è¯•
    print("\n1ï¸âƒ£ é¡ºåºå¤„ç†æµ‹è¯•")
    seq_result = test_batch_processing('ai_text_sentiment_analyze', reviews, api_key)
    print(f"  æ€»è€—æ—¶: {seq_result['total_time']:.2f}ç§’")
    print(f"  å¹³å‡è€—æ—¶: {seq_result['avg_time']:.2f}ç§’/è¯·æ±‚")
    
    # 2. å¹¶å‘å¤„ç†æµ‹è¯•
    print("\n2ï¸âƒ£ å¹¶å‘å¤„ç†æµ‹è¯•")
    for workers in [2, 5, 10]:
        conc_result = test_concurrent_requests(
            'ai_text_sentiment_analyze', reviews, api_key, max_workers=workers
        )
        print(f"  {workers}å¹¶å‘: æ€»è€—æ—¶{conc_result['total_time']:.2f}ç§’, "
              f"å¹³å‡{conc_result['avg_time']:.2f}ç§’/è¯·æ±‚")
    
    print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ!")


if __name__ == '__main__':
    main()
